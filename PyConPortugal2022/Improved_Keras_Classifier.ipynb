{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royn5618/Talks_Resources/blob/main/PyConPortugal2022/Improved_Keras_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8m5bAN40yyr"
      },
      "source": [
        "**About:**\n",
        "\n",
        "This notebook has an improved implmentation of an NLP classifier that predicts emotions using Keras Tuner with some other data and text based approaches for model improvement.\n",
        "\n",
        "**Data Source on Kaggle:**\n",
        "\n",
        "https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp\n",
        "\n",
        "**Data Source on HuggingFace:**\n",
        "\n",
        "https://huggingface.co/datasets/emotion\n",
        "\n",
        "**Recap Top 5 Techniques to Improve Model Performance**\n",
        "\n",
        " - Appending more data which eventually gives an ML model more examples to learn and generalize from.\n",
        " - Feature engineering for extracting helpful information from the given data that allow the model to easily and efficiently find patterns for predictions… and feature selection for treating the GIGO problem. Basically, this allows models to work with only a few useful features, remove noise, and save computation time and resources.\n",
        " - Try Multiple Algorithms to find the best-suited one for the predictions.\n",
        " - Use Cross-Validation for a robust and well-generalized model. Using cross-validation, you can train and test a model’s performance on multiple chunks of the dataset, get the average performances and figure out if a model is at its best or not.\n",
        " - Tune the Hyperparameters to identify the best combination suited for the dataset since they have a pivotal influence on the outcome of the model training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWmHzf8A1VlA"
      },
      "source": [
        "# Data import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IrmN8YI065k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvmu80Bs0d5-"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('Data/train.txt', sep=';', names=['text', 'emotion'])\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--3STFz21PJ-"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('Data/test.txt', sep=';', names=['text', 'emotion'])\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6umimDETyueN"
      },
      "outputs": [],
      "source": [
        "val_data = pd.read_csv('Data/val.txt', sep=';', names=['text', 'emotion'])\n",
        "val_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note \n",
        "\n",
        "I will be using train set to train and validation set to validate the model training process. Previously, I had retained 33% of the train data for validation. Hence, now I have more data to train and validate the model upon."
      ],
      "metadata": {
        "id": "W9qYfZFxzwM2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc0JxnR1dgl"
      },
      "source": [
        "# Data preparation\n",
        "## Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUPZycYQ1hXS"
      },
      "outputs": [],
      "source": [
        "train_data[\"emotion\"] = train_data[\"emotion\"].astype('category')\n",
        "train_data[\"emotion_label\"] = train_data[\"emotion\"].cat.codes\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx1q4sjD1UE4"
      },
      "outputs": [],
      "source": [
        "test_data[\"emotion\"] = test_data[\"emotion\"].astype('category')\n",
        "test_data[\"emotion_label\"] = test_data[\"emotion\"].cat.codes\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmXtU7aAyq-F"
      },
      "outputs": [],
      "source": [
        "val_data[\"emotion\"] = val_data[\"emotion\"].astype('category')\n",
        "val_data[\"emotion_label\"] = val_data[\"emotion\"].cat.codes\n",
        "val_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdwfWM3-3RU8"
      },
      "source": [
        "## One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO90VaQA3Q_m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CPXmRXZ1UBz"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels = train_data['text'], tf.one_hot(train_data[\"emotion_label\"], 6)\n",
        "test_features, test_labels = test_data['text'], tf.one_hot(test_data[\"emotion_label\"], 6)\n",
        "val_features, val_labels = val_data['text'], tf.one_hot(val_data[\"emotion_label\"], 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsvPoIuP1T_c"
      },
      "outputs": [],
      "source": [
        "train_features[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCmajMJe1T8l"
      },
      "outputs": [],
      "source": [
        "train_labels[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1i1Cz3w3uJH"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iusKE2o13vH-"
      },
      "outputs": [],
      "source": [
        "def get_labels_from_oh_code(oh_code):\n",
        "    \"\"\" Takes in one-hot encoded matrix\n",
        "    Returns a list of decoded categories\"\"\"\n",
        "    label_code = np.argmax(oh_code, axis=1)\n",
        "#     print(label_code)\n",
        "    label = test_data.emotion.cat.categories[label_code]\n",
        "#     print(list(label))\n",
        "    return list(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54BRuOk41T2k"
      },
      "outputs": [],
      "source": [
        "\"Test Method\"\n",
        "test= np.array(train_labels[:5])\n",
        "get_labels_from_oh_code(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iDU_gDBhPN_"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note \n",
        "\n",
        "I will be removing stopwords and stem the tokens. Stemming is crude heuristic process where variations of the same words/concepts are transformed into the root form. Stemmed tokens might not be a lexical word.\n",
        "\n",
        "Example: \n",
        "```\n",
        "Important -> import\n",
        "Imported -> import\n",
        "bravery -> braveri\n",
        "```"
      ],
      "metadata": {
        "id": "HEAADLl52FHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsyPJ978yj9I"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "STOPWORDS = stopwords.words('english')\n",
        "PORTER_STEMMER = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIcDGPHcyaVi"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    filtered_text = []\n",
        "    for each_word in word_tokenize(text):\n",
        "        if each_word not in STOPWORDS:\n",
        "            filtered_text.append(PORTER_STEMMER.stem(each_word))\n",
        "    return \" \".join(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXqjn_tmyaFZ"
      },
      "outputs": [],
      "source": [
        "train_data['text'] = train_data.text.apply(preprocess_text)\n",
        "test_data['text'] = test_data.text.apply(preprocess_text)\n",
        "val_data['text'] = val_data.text.apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTiil1p7r4WQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhiOGcMJhQtK"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "max_seq_len = 20\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words=vocab_size, lower=True)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(train_data['text'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "sequences_val = tokenizer.texts_to_sequences(val_data['text'])\n",
        "\n",
        "padded_train = pad_sequences(sequences_train, padding = 'post', maxlen=max_seq_len)\n",
        "padded_test = pad_sequences(sequences_test, padding = 'post', maxlen=max_seq_len)\n",
        "padded_val = pad_sequences(sequences_val, padding = 'post', maxlen=max_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHQ7b0Xdjnwq"
      },
      "source": [
        "# Model Designing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note\n",
        "\n",
        "I will be using hyperparameter tuning technique to select the best layer configurations for my ANN.\n",
        "\n",
        "**Hyperparameter Tuning**: choosing a set of optimal hyperparameters for a learning algorithm.\n",
        "\n",
        "- Random\n",
        "- Grid Search\n",
        "- Bayesian\n",
        "- Early-stopping based\n",
        "\n"
      ],
      "metadata": {
        "id": "lY_Tu0rC2JQO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdXKDIGij0Jw"
      },
      "source": [
        "## Build a Hyperband Model\n",
        "\n",
        "### Hyperband Tuner\n",
        "\n",
        "An extension of the Successive Halving Algorithm(SHA) for **adaptive resource allocation** with **early stopping** .\n",
        "\n",
        "**Successive Halving Algorithm:**\n",
        "1. Uniformly allocate all resources to the hyperparameter sets and tune them using half the resources/time.\n",
        "\n",
        "2. The top-half best performing set of hyperparameters is then “progressed” onto the next stage where the resulting models are trained with higher resources/time allocated to them.\n",
        "\n",
        "3. Repeat until there is only one configuration.\n",
        "\n",
        "\n",
        "**Hyperband Tuner** uses **η** which is the **rate of elimination** where only 1/ η of the hyperparameter sets are progressed to the next bracket for training and evaluation. η is determined by the formula ```rounded to nearest(1 + logbase=factor(max_epochs))``` in this Keras implementation of the algorithm.\n",
        "\n",
        "### Hyperparamters Chosen:\n",
        "```\n",
        "vector_size — Integer| range 100 to 500, step size: 100\n",
        "dropout_rate — Float | range 0.6 to 0.9, step size: 0.1\n",
        "lstm_units1 — Integer| range 32 to 512, step size: 32\n",
        "lstm_units2 — Integer| range 16 to 512, step size: 32\n",
        "learning_rate — Choice| 1e-2, 1e-3, 1e-4\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIUXsD7-rx2G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esPG37Xc7zMH"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr79RIa07zp0"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uns7tQ9q1TzP"
      },
      "outputs": [],
      "source": [
        "vector_size = 300\n",
        "\n",
        "def model_builder(hp):\n",
        "    model = Sequential()\n",
        "    hp_vector_size = hp.Int('vector_size', min_value=100, max_value=500, step=100)\n",
        "    model.add(\n",
        "        Embedding(input_dim=vocab_size,\n",
        "                  output_dim=vector_size,\n",
        "                  input_length=max_seq_len))\n",
        "    hp_dropout_rate = hp.Float('dropout_rate', min_value=0.6, max_value=0.9, step=0.1)\n",
        "    model.add(Dropout(hp_dropout_rate))\n",
        "    hp_lstm_units1 = hp.Int('lstm_units1', min_value=32, max_value=512, step=32)\n",
        "    model.add(LSTM(hp_lstm_units1,return_sequences=True))\n",
        "    hp_lstm_units2 = hp.Int('lstm_units2', min_value=16, max_value=512, step=32)\n",
        "    model.add(LSTM(hp_lstm_units2))\n",
        "    model.add(Dense(6,activation='softmax'))\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09WNBofDzL4Y"
      },
      "outputs": [],
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective=kt.Objective(\"val_recall\", direction=\"max\"),\n",
        "                     max_epochs=20,\n",
        "                     factor=3,\n",
        "                     directory=\"model_trials_1\",\n",
        "                     project_name=\"emotion_detector_1\"\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8X9V8-KjtKY"
      },
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              mode='min', \n",
        "                                              patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N231u59hzVNy"
      },
      "outputs": [],
      "source": [
        "tuner.search(padded_train, \n",
        "             train_labels, \n",
        "             epochs=20, \n",
        "             validation_data=(padded_val, val_labels), \n",
        "             callbacks=[stop_early]\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJbt-duzVKN"
      },
      "outputs": [],
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w_JjcRNzVHv"
      },
      "outputs": [],
      "source": [
        "best_hps.get('vector_size')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhpkrLhjzao1"
      },
      "outputs": [],
      "source": [
        "best_hps.get('dropout_rate')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNJn0Azvz2s0"
      },
      "source": [
        "## Build Model with Best HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAIBFitozag4"
      },
      "outputs": [],
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model_best_hp = tuner.hypermodel.build(best_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haIrLNzh7ZjK"
      },
      "outputs": [],
      "source": [
        "model_best_hp.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUscmjNcpzDI"
      },
      "source": [
        "### Set Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPGfg4KezzFR"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                  mode='min',\n",
        "                                  patience=5,\n",
        "                                  verbose=1,\n",
        "                                  restore_best_weights=True)  \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgwNcsegq0Bm"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa5Tg2yzkX_X"
      },
      "outputs": [],
      "source": [
        "history = model_best_hp.fit(padded_train, \n",
        "                            train_labels, \n",
        "                            epochs=20,\n",
        "                            callbacks=callbacks,\n",
        "                            validation_data=(padded_val, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_best_hp.metrics"
      ],
      "metadata": {
        "id": "Tcl1kRFE_CdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eFqKnt6q2Nb"
      },
      "source": [
        "## Visualize and verify the Loss per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmUB53pKt3q2"
      },
      "outputs": [],
      "source": [
        "from plotly.graph_objs import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdshwS-DkX9G"
      },
      "outputs": [],
      "source": [
        "metric_to_plot = \"loss\"\n",
        "epochs = list(range(1, max(history.epoch) + 2))\n",
        "training_loss = history.history[metric_to_plot]\n",
        "validation_loss = history.history[\"val_\" + metric_to_plot]\n",
        "\n",
        "trace1 = {\n",
        "    \"mode\": \"lines+markers\",\n",
        "    \"name\": \"Training Loss\",\n",
        "    \"type\": \"scatter\",\n",
        "    \"x\": epochs,\n",
        "    \"y\": training_loss\n",
        "}\n",
        "\n",
        "trace2 = {\n",
        "    \"mode\": \"lines+markers\",\n",
        "    \"name\": \"Validation Loss\",\n",
        "    \"type\": \"scatter\",\n",
        "    \"x\": epochs,\n",
        "    \"y\": validation_loss\n",
        "}\n",
        "\n",
        "data = Data([trace1, trace2])\n",
        "layout = {\n",
        "    \"title\": \"Training - Validation Loss\",\n",
        "    \"xaxis\": {\n",
        "        \"title\": \"Number of epochs\",\n",
        "        \"titlefont\": {\n",
        "            \"size\": 18,\n",
        "            \"color\": \"#7f7f7f\"\n",
        "        }\n",
        "    },\n",
        "    \"yaxis\": {\n",
        "        \"title\": \"Loss\",\n",
        "        \"titlefont\": {\n",
        "            \"size\": 18,\n",
        "            \"color\": \"#7f7f7f\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "fig = Figure(data=data, layout=layout)\n",
        "fig.update_layout(hovermode=\"x unified\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSQy-uV9rDKM"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxVbguuPrpuA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlZbvBXmrYxA"
      },
      "outputs": [],
      "source": [
        "y_pred_one_hot_encoded = (model_best_hp.predict(padded_train)> 0.5).astype(\"int32\")\n",
        "y_pred_one_hot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4eT2NMOrYtn"
      },
      "outputs": [],
      "source": [
        "y_pred = np.array(tf.argmax(y_pred_one_hot_encoded, axis=1))\n",
        "print(classification_report(train_data['emotion_label'], y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTourMYUrYre"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation on Test Data\n",
        "y_pred_one_hot_encoded = (model_best_hp.predict(padded_test)> 0.5).astype(\"int32\")\n",
        "y_pred = np.array(tf.argmax(y_pred_one_hot_encoded, axis=1))\n",
        "print(classification_report(test_data['emotion_label'], y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note\n",
        "\n",
        " - Cross validation for deep learning models is computationally expensive, and time consuming. You can use KFolds or Stratified Folds(for imbalanced data) and run model training for each fold and then compare scores.\n",
        " - Use other RNN layers instead of LSTM, other architecture designs as well to check the performance. Some experiments are here: https://github.com/royn5618/Medium_Blog_Codes/tree/master/Emotion%20Detection"
      ],
      "metadata": {
        "id": "A5yg709-3KFu"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VCfLIp5Op7CC",
        "hgwNcsegq0Bm",
        "-eFqKnt6q2Nb",
        "b4ZE0teWq_I5"
      ],
      "authorship_tag": "ABX9TyOspZkQBEMFkeWaya520X9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}